# ğŸ¤– LLM-WebApp â€” Chat con Gemini AI

![Python](https://img.shields.io/badge/Python-3776AB?style=for-the-badge&logo=python&logoColor=white)
![Flask](https://img.shields.io/badge/Flask-000000?style=for-the-badge&logo=flask&logoColor=white)
![Google Gemini](https://img.shields.io/badge/Google_Gemini-8E44AD?style=for-the-badge&logo=google&logoColor=white)
![HTML5](https://img.shields.io/badge/HTML5-E34F26?style=for-the-badge&logo=html5&logoColor=white)
![CSS3](https://img.shields.io/badge/CSS3-1572B6?style=for-the-badge&logo=css3&logoColor=white)
![Docker](https://img.shields.io/badge/Docker-2496ED?style=for-the-badge&logo=docker&logoColor=white)
![Heroku](https://img.shields.io/badge/Heroku-430098?style=for-the-badge&logo=heroku&logoColor=white)

Interfaz de chat web construida con **Flask** para interactuar con los modelos de Google Gemini. Permite ajustar parÃ¡metros de generaciÃ³n, subir archivos PDF como contexto y ver las respuestas renderizadas en Markdown.

---

## âœ¨ CaracterÃ­sticas

- **ğŸ¤– Modelos de Ãºltima generaciÃ³n:** `gemini-3-flash-preview` y `gemini-3-pro-preview`.
- **âš™ï¸ Control de parÃ¡metros:** AjustÃ¡ `temperatura`, `top-p`, `top-k` y `max tokens` antes de iniciar el chat.
- **ğŸ“„ Chat sobre PDFs:** SubÃ­ un archivo PDF en el primer mensaje; Gemini lo recibirÃ¡ en formato nativo (base64 + mime_type) y podrÃ¡ responder preguntas sobre su contenido.
- **ğŸ‘ï¸ Renderizado Markdown:** Las respuestas del modelo se muestran con formato completo: listas, bloques de cÃ³digo, tablas, headings, etc.
- **ğŸ’¾ Sesiones en servidor:** La sesiÃ³n se almacena en el filesystem del servidor (Flask-Session), no en cookies, evitando el lÃ­mite de 4 KB y permitiendo manejar PDFs grandes.
- **ï¿½ Reset de conversaciÃ³n:** BotÃ³n para reiniciar el chat y cambiar de modelo o PDF.

---

## ğŸ› ï¸ Stack TecnolÃ³gico

| Capa | TecnologÃ­a |
|---|---|
| Backend | Python 3.11+ Â· Flask 3 Â· Flask-Session |
| IA | Google Gemini (SDK `google-genai`) |
| Frontend | HTML5 Â· CSS3 Â· JavaScript Vanilla |
| Markdown | [Marked.js](https://marked.js.org/) (CDN) |
| Servidor prod. | Gunicorn |
| Contenedor | Docker Â· Heroku |

---

## ğŸ EjecuciÃ³n Local

### Requisitos previos

- Python 3.11+
- Una [API Key de Google AI Studio](https://aistudio.google.com/app/apikey)

### InstalaciÃ³n

```bash
# 1. Clonar el repositorio
git clone https://github.com/facuberon/LLM-WebApp.git
cd LLM-WebApp

# 2. Instalar dependencias
pip install -r requirements.txt


# 3. Iniciar la app
python app.py
```

AbrÃ­ **http://localhost:5000** en tu navegador.

> Si no configurÃ¡s un archivo `.env`, podÃ©s ingresar la API Key directamente en el panel lateral de la app al iniciar el primer chat.

---

## ğŸ“– GuÃ­a de Uso

1. ğŸ”‘ **API Key:** Si no la configuraste en `.env`, pegala en el campo del sidebar.
2. ğŸ¤– **Modelo:** ElegÃ­ entre `gemini-3-flash-preview` (mÃ¡s rÃ¡pido) o `gemini-3-pro-preview` (mÃ¡s capaz).
3. ğŸ“„ **PDF (opcional):** SubÃ­ un archivo PDF antes del primer mensaje. Gemini lo recibirÃ¡ como documento nativo.
4. ğŸ›ï¸ **ParÃ¡metros:** AjustÃ¡ temperatura, top-p, top-k y max tokens segÃºn necesites. Se bloquean una vez iniciada la conversaciÃ³n.
5. âŒ¨ï¸ **Chat:** EscribÃ­ tu mensaje y presionÃ¡ `Enter` (o `Shift+Enter` para nueva lÃ­nea).
6. ğŸ”„ **Resetear:** HacÃ© clic en "â†º Reiniciar chat" para empezar una nueva conversaciÃ³n con configuraciÃ³n diferente.

---

## ğŸ³ Docker

```bash
# Construir la imagen
docker build -t llm-webapp .

# Ejecutar el contenedor (pasando la API Key como variable de entorno)
docker run -d -p 5000:5000 -e API_KEY=tu_api_key llm-webapp
```

La app estarÃ¡ en `http://localhost:5000`.

---

## â˜ï¸ Deploy en Heroku

```bash
heroku container:login
heroku create nombre-app
heroku config:set API_KEY=tu_api_key -a nombre-app
heroku container:push web -a nombre-app
heroku container:release web -a nombre-app
```

---

## ğŸ“‚ Estructura del Proyecto

```
ğŸ“¦ LLM-WebApp
 â”£ ğŸ“œ app.py              # AplicaciÃ³n Flask (rutas, lÃ³gica, integraciÃ³n Gemini)
 â”£ ğŸ“‚ templates/
 â”ƒ â”— ğŸ“œ index.html        # Interfaz de chat (HTML + CSS + JS + Marked.js)
 â”£ ğŸ“‚ .flask_sessions/    # Archivos de sesiÃ³n del servidor (generado automÃ¡ticamente, en .gitignore)
 â”£ ï¿½ model.ipynb         # Notebook de pruebas de la API de Gemini
 â”£ ğŸ“œ Dockerfile          # Imagen Docker con Gunicorn
 â”£ ğŸ“œ Procfile            # Comando de arranque para Heroku
 â”£ ğŸ“œ requirements.txt    # Dependencias de Python
 â”— ğŸ“œ README.md
```

---

## ğŸ’¡ PrÃ³ximas Mejoras

- [ ] Soporte para mÃ¡s formatos de archivo (`.txt`, `.docx`).
- [ ] OpciÃ³n para exportar la conversaciÃ³n en Markdown o PDF.
- [ ] Historial de conversaciones mÃºltiples (multi-sesiÃ³n).
- [ ] Syntax highlighting en bloques de cÃ³digo.
- [ ] MÃ¡s opciones de configuraciÃ³n de modelos.

---

Hecho con â¤ï¸ y Python.
